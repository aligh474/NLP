{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an example from coursera projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import trax \n",
    "from trax import layers as tl\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "* geo: geographical entity\n",
    "* org: organization\n",
    "* per: person \n",
    "* gpe: geopolitical entity\n",
    "* tim: time indicator\n",
    "* art: artifact\n",
    "* eve: event\n",
    "* nat: natural phenomenon\n",
    "* O: filler word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(vocab_path, tags_path):\n",
    "    vocab = {}\n",
    "    with open(vocab_path) as f:\n",
    "        for i, l in enumerate(f.read().splitlines()):\n",
    "            vocab[l] = i  \n",
    "    vocab['<PAD>'] = len(vocab)\n",
    "    tag_map = {}\n",
    "    with open(tags_path) as f:\n",
    "        for i, t in enumerate(f.read().splitlines()):\n",
    "            tag_map[t] = i \n",
    "    \n",
    "    return vocab, tag_map\n",
    "\n",
    "def get_params(vocab, tag_map, sentences_file, labels_file):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    with open(sentences_file) as f:\n",
    "        for sentence in f.read().splitlines():\n",
    "            s = [vocab[token] if token in vocab \n",
    "                 else vocab['UNK']\n",
    "                 for token in sentence.split(' ')]\n",
    "            sentences.append(s)\n",
    "\n",
    "    with open(labels_file) as f:\n",
    "        for sentence in f.read().splitlines():\n",
    "            l = [tag_map[label] for label in sentence.split(' ')] \n",
    "            labels.append(l) \n",
    "    return sentences, labels, len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, tag_map = get_vocab('data/large/words.txt', 'data/large/tags.txt')\n",
    "t_sentences, t_labels, t_size = get_params(vocab, tag_map, 'data/large/train/sentences.txt', 'data/large/train/labels.txt')\n",
    "v_sentences, v_labels, v_size = get_params(vocab, tag_map, 'data/large/val/sentences.txt', 'data/large/val/labels.txt')\n",
    "test_sentences, test_labels, test_size = get_params(vocab, tag_map, 'data/large/test/sentences.txt', 'data/large/test/labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61, 8, 62, 63, 9, 64, 1, 9, 65, 66, 1, 67, 68, 69, 70, 71, 11, 9, 72, 73, 74, 75, 1, 76, 21] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 5, 6, 0, 0, 0, 2, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(t_sentences[4],t_labels[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data_generator(batch_size, max_length, x, y, padding):\n",
    "    index =[]\n",
    "    while True:\n",
    "        if len(index)<len(x):\n",
    "            #find index of lines less than maxlength\n",
    "            index = np.where([1 if len(line)<max_length else 0 for line in x])[0]\n",
    "        batch_index = np.random.choice(index,batch_size)\n",
    "        #remove used index \n",
    "        index = [x for x in index if x not in batch_index]\n",
    "        #make a batch\n",
    "        batch_x = [x[i] for i in batch_index]  \n",
    "        batch_y = [y[i] for i in batch_index]  \n",
    "        batch_x_ = []\n",
    "        batch_y_ = []\n",
    "        # make a tensor\n",
    "        for li in batch_x:\n",
    "            pad = [padding] * (max_length - len(li))\n",
    "            li_pad = li + pad\n",
    "            batch_x_.append(li_pad)\n",
    "            \n",
    "        for li in batch_y:\n",
    "            pad = [padding] * (max_length - len(li))\n",
    "            li_pad = li + pad\n",
    "            batch_y_.append(li_pad)\n",
    "\n",
    "        X = np.array(batch_x_)\n",
    "        Y = np.array(batch_y_)\n",
    "        yield X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "mini_sentences = t_sentences[0: 8]\n",
    "mini_labels = t_labels[0: 8]\n",
    "dg = batch_data_generator(batch_size, 60,mini_sentences, mini_labels, vocab[\"<PAD>\"])\n",
    "X1, Y1 = next(dg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   61,     6,    85,    86,    87,     1,    88,    89,    90,\n",
       "           11,    91,    92,    93,    94,    95,    93,    96,    93,\n",
       "           13,    97,    21, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "        35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "        35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "        35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "        35180, 35180, 35180, 35180, 35180, 35180],\n",
       "       [   61,    77,    78,    79,    80,    67,    68,    81,    11,\n",
       "            9,    12,    25,    13,     9,    82,    83,     1,    84,\n",
       "           16,    17,    11,    19,    20,    21, 35180, 35180, 35180,\n",
       "        35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "        35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "        35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "        35180, 35180, 35180, 35180, 35180, 35180]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NER(vocab_size=35181, d_model=50, tags=tag_map):\n",
    "\n",
    "    model = tl.Serial(\n",
    "      tl.Embedding(vocab_size, d_model), \n",
    "      tl.LSTM(d_model), \n",
    "      tl.Dense(len(tags)), \n",
    "      tl.LogSoftmax()  \n",
    "      )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial[\n",
      "  Embedding_35181_50\n",
      "  LSTM_50\n",
      "  Dense_17\n",
      "  LogSoftmax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "model = NER()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trax.supervised import training\n",
    "\n",
    "rnd.seed(33)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_generator = trax.data.inputs.add_loss_weights(\n",
    "    batch_data_generator(batch_size,70, t_sentences, t_labels, vocab['<PAD>']),\n",
    "    id_to_mask=vocab['<PAD>'])\n",
    "\n",
    "\n",
    "eval_generator = trax.data.inputs.add_loss_weights(\n",
    "    batch_data_generator(batch_size,70, v_sentences, v_labels, vocab['<PAD>']),\n",
    "    id_to_mask=vocab['<PAD>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(NER, train_generator, eval_generator, train_steps=1, output_dir='model'):\n",
    "\n",
    "    train_task = training.TrainTask(\n",
    "      train_generator, \n",
    "      loss_layer = tl.CrossEntropyLoss(),\n",
    "      optimizer = trax.optimizers.Adam(0.01), \n",
    "    )\n",
    "\n",
    "    eval_task = training.EvalTask(\n",
    "      labeled_data = eval_generator, \n",
    "      metrics = [tl.CrossEntropyLoss(), tl.Accuracy()], \n",
    "      n_eval_batches = 10  \n",
    "    )\n",
    "\n",
    "    training_loop = training.Loop(\n",
    "        NER, \n",
    "        train_task, \n",
    "        eval_tasks = eval_task,\n",
    "        output_dir = output_dir) \n",
    "\n",
    "    \n",
    "    training_loop.run(n_steps = train_steps)\n",
    "\n",
    "    return training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step   1300: Ran 100 train steps in 10.47 secs\n",
      "Step   1300: train CrossEntropyLoss |  0.09662773\n",
      "Step   1300: eval  CrossEntropyLoss |  0.14989577\n",
      "Step   1300: eval          Accuracy |  0.95700048\n",
      "\n",
      "Step   1400: Ran 100 train steps in 9.25 secs\n",
      "Step   1400: train CrossEntropyLoss |  0.09209779\n",
      "Step   1400: eval  CrossEntropyLoss |  0.14010059\n",
      "Step   1400: eval          Accuracy |  0.95857831\n",
      "\n",
      "Step   1500: Ran 100 train steps in 9.25 secs\n",
      "Step   1500: train CrossEntropyLoss |  0.08418070\n",
      "Step   1500: eval  CrossEntropyLoss |  0.13821157\n",
      "Step   1500: eval          Accuracy |  0.95946410\n",
      "\n",
      "Step   1600: Ran 100 train steps in 9.29 secs\n",
      "Step   1600: train CrossEntropyLoss |  0.07763737\n",
      "Step   1600: eval  CrossEntropyLoss |  0.13990689\n",
      "Step   1600: eval          Accuracy |  0.95940822\n",
      "\n",
      "Step   1700: Ran 100 train steps in 9.43 secs\n",
      "Step   1700: train CrossEntropyLoss |  0.07715777\n",
      "Step   1700: eval  CrossEntropyLoss |  0.14950747\n",
      "Step   1700: eval          Accuracy |  0.95550586\n",
      "\n",
      "Step   1800: Ran 100 train steps in 9.31 secs\n",
      "Step   1800: train CrossEntropyLoss |  0.07195080\n",
      "Step   1800: eval  CrossEntropyLoss |  0.14107267\n",
      "Step   1800: eval          Accuracy |  0.95998499\n",
      "\n",
      "Step   1900: Ran 100 train steps in 9.42 secs\n",
      "Step   1900: train CrossEntropyLoss |  0.07151753\n",
      "Step   1900: eval  CrossEntropyLoss |  0.14677472\n",
      "Step   1900: eval          Accuracy |  0.95914525\n",
      "\n",
      "Step   2000: Ran 100 train steps in 9.27 secs\n",
      "Step   2000: train CrossEntropyLoss |  0.07202168\n",
      "Step   2000: eval  CrossEntropyLoss |  0.16419446\n",
      "Step   2000: eval          Accuracy |  0.95466148\n",
      "\n",
      "Step   2100: Ran 100 train steps in 9.33 secs\n",
      "Step   2100: train CrossEntropyLoss |  0.06873727\n",
      "Step   2100: eval  CrossEntropyLoss |  0.16658918\n",
      "Step   2100: eval          Accuracy |  0.95370250\n",
      "\n",
      "Step   2200: Ran 100 train steps in 9.36 secs\n",
      "Step   2200: train CrossEntropyLoss |  0.06482762\n",
      "Step   2200: eval  CrossEntropyLoss |  0.13448917\n",
      "Step   2200: eval          Accuracy |  0.95971220\n",
      "\n",
      "Step   2300: Ran 100 train steps in 9.41 secs\n",
      "Step   2300: train CrossEntropyLoss |  0.06370392\n",
      "Step   2300: eval  CrossEntropyLoss |  0.15097249\n",
      "Step   2300: eval          Accuracy |  0.95479845\n",
      "\n",
      "Step   2400: Ran 100 train steps in 9.42 secs\n",
      "Step   2400: train CrossEntropyLoss |  0.06118181\n",
      "Step   2400: eval  CrossEntropyLoss |  0.15769587\n",
      "Step   2400: eval          Accuracy |  0.95923309\n",
      "\n",
      "Step   2500: Ran 100 train steps in 9.31 secs\n",
      "Step   2500: train CrossEntropyLoss |  0.06135781\n",
      "Step   2500: eval  CrossEntropyLoss |  0.15932310\n",
      "Step   2500: eval          Accuracy |  0.95891893\n",
      "\n",
      "Step   2600: Ran 100 train steps in 9.33 secs\n",
      "Step   2600: train CrossEntropyLoss |  0.05876499\n",
      "Step   2600: eval  CrossEntropyLoss |  0.16564866\n",
      "Step   2600: eval          Accuracy |  0.95493134\n",
      "\n",
      "Step   2700: Ran 100 train steps in 9.44 secs\n",
      "Step   2700: train CrossEntropyLoss |  0.05946331\n",
      "Step   2700: eval  CrossEntropyLoss |  0.13534616\n",
      "Step   2700: eval          Accuracy |  0.96094795\n",
      "\n",
      "Step   2800: Ran 100 train steps in 9.39 secs\n",
      "Step   2800: train CrossEntropyLoss |  0.05609615\n",
      "Step   2800: eval  CrossEntropyLoss |  0.12839868\n",
      "Step   2800: eval          Accuracy |  0.96335362\n",
      "\n",
      "Step   2900: Ran 100 train steps in 9.40 secs\n",
      "Step   2900: train CrossEntropyLoss |  0.05498687\n",
      "Step   2900: eval  CrossEntropyLoss |  0.15983905\n",
      "Step   2900: eval          Accuracy |  0.95549567\n",
      "\n",
      "Step   3000: Ran 100 train steps in 9.40 secs\n",
      "Step   3000: train CrossEntropyLoss |  0.05368745\n",
      "Step   3000: eval  CrossEntropyLoss |  0.15138697\n",
      "Step   3000: eval          Accuracy |  0.95909160\n",
      "\n",
      "Step   3100: Ran 100 train steps in 9.44 secs\n",
      "Step   3100: train CrossEntropyLoss |  0.05364841\n",
      "Step   3100: eval  CrossEntropyLoss |  0.14559296\n",
      "Step   3100: eval          Accuracy |  0.96040189\n",
      "\n",
      "Step   3200: Ran 100 train steps in 9.33 secs\n",
      "Step   3200: train CrossEntropyLoss |  0.05172148\n",
      "Step   3200: eval  CrossEntropyLoss |  0.14762411\n",
      "Step   3200: eval          Accuracy |  0.96077041\n"
     ]
    }
   ],
   "source": [
    "training_loop = train_model(NER(), train_generator, eval_generator, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NER()\n",
    "model.init(trax.shapes.ShapeDtype((1, 1), dtype=np.int32))\n",
    "model.init_from_file('model/model.pkl.gz', weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9596766\n"
     ]
    }
   ],
   "source": [
    "def evaluate_prediction(pred, labels, pad):\n",
    "\n",
    "    outputs = np.argmax(pred, axis=2)\n",
    "    mask = labels != pad\n",
    "    accuracy = np.sum(outputs == labels) / float(np.sum(mask))\n",
    "    return accuracy\n",
    "\n",
    "x, y = next(batch_data_generator(len(test_sentences),70, test_sentences, test_labels, vocab['<PAD>']))\n",
    "accuracy = evaluate_prediction(model(x), y, vocab['<PAD>'])\n",
    "print(\"accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
