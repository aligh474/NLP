{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import trax\n",
    "import trax.fastmath.numpy as np\n",
    "import pickle\n",
    "import itertools\n",
    "import numpy\n",
    "import random as rnd\n",
    "from trax import fastmath\n",
    "from trax import layers as tl\n",
    "\n",
    "# set random seed\n",
    "rnd.seed(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 125097\n",
      "hath beaten down young hotspur and his troops,\n"
     ]
    }
   ],
   "source": [
    "dirname = 'data/'\n",
    "lines = [] \n",
    "for filename in os.listdir(dirname):\n",
    "    with open(os.path.join(dirname, filename)) as files:\n",
    "        for line in files:\n",
    "            # remove leading and trailing whitespace\n",
    "            pure_line = line.strip()\n",
    "            # if pure_line is not the empty string,\n",
    "            if pure_line:\n",
    "                lines.append(pure_line.lower())\n",
    "                \n",
    "\n",
    "print(f\"Number of lines: {len(lines)}\")\n",
    "print(lines[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make training and eval from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines for training: 124097\n",
      "Number of lines for validation: 1000\n"
     ]
    }
   ],
   "source": [
    "eval_lines = lines[-1000:] \n",
    "lines = lines[:-1000] \n",
    "\n",
    "print(f\"Number of lines for training: {len(lines)}\")\n",
    "print(f\"Number of lines for validation: {len(eval_lines)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make tensor from each letter(convert to ascii code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_to_tensor(line, EOS_int=1):\n",
    "    \n",
    "    tensor = []\n",
    "    # for each character:\n",
    "    for c in line:\n",
    "        c_int = ord(c)\n",
    "        tensor.append(c_int)\n",
    "\n",
    "    tensor.append(EOS_int)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[73,\n",
       " 32,\n",
       " 108,\n",
       " 105,\n",
       " 107,\n",
       " 101,\n",
       " 32,\n",
       " 100,\n",
       " 101,\n",
       " 101,\n",
       " 112,\n",
       " 108,\n",
       " 101,\n",
       " 97,\n",
       " 114,\n",
       " 110,\n",
       " 105,\n",
       " 110,\n",
       " 103,\n",
       " 33,\n",
       " 1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_to_tensor('I like deeplearning!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data_generator(batch_size, max_length, data_lines, line_to_tensor=line_to_tensor):\n",
    "    index =[]\n",
    "    while True:\n",
    "        if len(index)<len(data_lines):\n",
    "            #find index of lines less than maxlength\n",
    "            index = numpy.where([1 if len(line)<max_length else 0 for line in data_lines])[0]\n",
    "        batch_index = numpy.random.choice(index,batch_size)\n",
    "        #remove used index \n",
    "        index = [x for x in index if x not in batch_index]\n",
    "        #make a batch\n",
    "        batch = [data_lines[i] for i in batch_index]  \n",
    "\n",
    "        batch_ = []\n",
    "        mask = []\n",
    "        # make a tensor\n",
    "        for li in batch:\n",
    "            tensor = line_to_tensor(li)\n",
    "            pad = [0] * (max_length - len(tensor))\n",
    "            tensor_pad = tensor + pad\n",
    "            example_mask = [0 if t == 0 else 1 for t in tensor_pad]\n",
    "            mask.append(example_mask)\n",
    "            batch_.append(tensor_pad)\n",
    "        batch_np_arr = np.array(batch_)\n",
    "        mask_np_arr = np.array(mask)\n",
    "        yield batch_np_arr,batch_np_arr,mask_np_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DeviceArray([[51, 52, 53, 54, 55, 56, 57, 48, 49,  1,  0,  0,  0,  0,  0],\n",
       "              [50, 51, 52, 53, 54, 57, 48,  1,  0,  0,  0,  0,  0,  0,  0]],            dtype=int32),\n",
       " DeviceArray([[51, 52, 53, 54, 55, 56, 57, 48, 49,  1,  0,  0,  0,  0,  0],\n",
       "              [50, 51, 52, 53, 54, 57, 48,  1,  0,  0,  0,  0,  0,  0,  0]],            dtype=int32),\n",
       " DeviceArray([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "              [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]], dtype=int32))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try out batch data generator\n",
    "tmp_lines = ['12345678901', #length 11\n",
    "             '123456789', # length 9\n",
    "             '2345690', # length 9\n",
    "             '345678901'] # length 9\n",
    "\n",
    "# Get a batch size of 2, max length 10\n",
    "tmp_data_gen = batch_data_generator(batch_size=2, \n",
    "                              max_length=15, \n",
    "                              data_lines=tmp_lines,\n",
    "                            )\n",
    "\n",
    "# get one batch\n",
    "tmp_batch = next(tmp_data_gen)\n",
    "\n",
    "# view the batch\n",
    "tmp_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRULM(vocab_size=256, d_model=512, n_layers=2, mode='train'):\n",
    "    model = tl.Serial(\n",
    "      tl.ShiftRight(mode=mode), # Stack the ShiftRight layer\n",
    "      tl.Embedding(vocab_size=vocab_size, d_feature=d_model), # Stack the embedding layer\n",
    "      [tl.GRU(n_units=d_model) for _ in range(n_layers)], # Stack GRU layers of d_model units keeping n_layer parameter in mind (use list comprehension syntax)\n",
    "      tl.Dense(n_units=vocab_size), # Dense layer\n",
    "      tl.LogSoftmax() # Log Softmax\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial[\n",
      "  Serial[\n",
      "    ShiftRight(1)\n",
      "  ]\n",
      "  Embedding_256_512\n",
      "  GRU_512\n",
      "  GRU_512\n",
      "  Dense_256\n",
      "  LogSoftmax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "model = GRULM()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trax.supervised import training\n",
    "\n",
    "\n",
    "def train_model(model, data_generator, batch_size=32, max_length=64, lines=lines, eval_lines=eval_lines, n_steps=1, output_dir='model/'): \n",
    "\n",
    "    bare_train_generator = data_generator(batch_size, max_length, data_lines=lines)\n",
    "    infinite_train_generator = itertools.cycle(bare_train_generator)\n",
    "    \n",
    "    bare_eval_generator = data_generator(batch_size, max_length, data_lines=eval_lines)\n",
    "    infinite_eval_generator = itertools.cycle(bare_eval_generator)\n",
    "   \n",
    "    train_task = training.TrainTask(\n",
    "        labeled_data=infinite_train_generator, \n",
    "        loss_layer=tl.CrossEntropyLoss(),   \n",
    "        optimizer=trax.optimizers.Adam(0.0005)     \n",
    "    )\n",
    "\n",
    "    eval_task = training.EvalTask(\n",
    "        labeled_data=infinite_eval_generator,    \n",
    "        metrics=[tl.CrossEntropyLoss(), tl.Accuracy()], \n",
    "        n_eval_batches=3      \n",
    "    )\n",
    "    \n",
    "    training_loop = training.Loop(model,\n",
    "                                  tasks = train_task,\n",
    "                                  eval_tasks=eval_task,\n",
    "                                  output_dir=output_dir)\n",
    "\n",
    "    training_loop.run(n_steps=n_steps)\n",
    "    \n",
    "    return training_loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step  20400: Ran 100 train steps in 46.80 secs\n",
      "Step  20400: train CrossEntropyLoss |  1.32728612\n",
      "Step  20400: eval  CrossEntropyLoss |  1.39446664\n",
      "Step  20400: eval          Accuracy |  0.55682667\n",
      "\n",
      "Step  20500: Ran 100 train steps in 44.33 secs\n",
      "Step  20500: train CrossEntropyLoss |  1.28492093\n",
      "Step  20500: eval  CrossEntropyLoss |  1.35898980\n",
      "Step  20500: eval          Accuracy |  0.57404141\n",
      "\n",
      "Step  20600: Ran 100 train steps in 44.14 secs\n",
      "Step  20600: train CrossEntropyLoss |  1.26225626\n",
      "Step  20600: eval  CrossEntropyLoss |  1.33000886\n",
      "Step  20600: eval          Accuracy |  0.58211164\n",
      "\n",
      "Step  20700: Ran 100 train steps in 44.08 secs\n",
      "Step  20700: train CrossEntropyLoss |  1.24394274\n",
      "Step  20700: eval  CrossEntropyLoss |  1.35478957\n",
      "Step  20700: eval          Accuracy |  0.57309226\n",
      "\n",
      "Step  20800: Ran 100 train steps in 44.08 secs\n",
      "Step  20800: train CrossEntropyLoss |  1.23405206\n",
      "Step  20800: eval  CrossEntropyLoss |  1.26914660\n",
      "Step  20800: eval          Accuracy |  0.59417073\n"
     ]
    }
   ],
   "source": [
    "training_loop = train_model(GRULM(),batch_data_generator,n_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(preds, target):\n",
    "    \"\"\"Function to test the model.\n",
    "\n",
    "    Args:\n",
    "        preds (jax.interpreters.xla.DeviceArray): Predictions of a list of batches of tensors corresponding to lines of text.\n",
    "        target (jax.interpreters.xla.DeviceArray): Actual list of batches of tensors corresponding to lines of text.\n",
    "\n",
    "    Returns:\n",
    "        float: log_perplexity of the model.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    total_log_ppx = np.sum(preds * tl.one_hot(target, preds.shape[-1]),axis= -1) # HINT: tl.one_hot() should replace one of the Nones\n",
    "\n",
    "    non_pad = 1.0 - np.equal(target, 0)          # You should check if the target equals 0\n",
    "    ppx = total_log_ppx * non_pad                       # Get rid of the padding\n",
    "\n",
    "    log_ppx = np.sum(ppx) / np.sum(non_pad)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return -log_ppx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log perplexity and perplexity of your model are respectively 1.2325175 3.4298532\n",
      "The log perplexity and perplexity of your model are respectively 1.2144725 3.368517\n",
      "The log perplexity and perplexity of your model are respectively 1.1800188 3.2544353\n",
      "The log perplexity and perplexity of your model are respectively 1.2031776 3.3306837\n",
      "The log perplexity and perplexity of your model are respectively 1.2331581 3.4320512\n",
      "The log perplexity and perplexity of your model are respectively 1.2275819 3.4129665\n",
      "The log perplexity and perplexity of your model are respectively 1.3284045 3.7750158\n",
      "The log perplexity and perplexity of your model are respectively 1.1782154 3.2485716\n",
      "The log perplexity and perplexity of your model are respectively 1.2500899 3.4906566\n",
      "The log perplexity and perplexity of your model are respectively 1.1415116 3.1314983\n"
     ]
    }
   ],
   "source": [
    "model = GRULM()\n",
    "batch_size =32\n",
    "max_length=64\n",
    "model.init_from_file('model/model.pkl.gz')\n",
    "for x in range(10):\n",
    "    batch = next(batch_data_generator(batch_size, max_length, lines))\n",
    "    preds = model(batch[0])\n",
    "    log_ppx = test_model(preds, batch[1])\n",
    "    print('The log perplexity and perplexity of your model are respectively', log_ppx, np.exp(log_ppx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
