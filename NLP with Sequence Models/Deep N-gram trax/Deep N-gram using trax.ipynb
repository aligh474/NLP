{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import trax\n",
    "import trax.fastmath.numpy as np\n",
    "import pickle\n",
    "import itertools\n",
    "import numpy\n",
    "import random as rnd\n",
    "from trax import fastmath\n",
    "from trax import layers as tl\n",
    "\n",
    "# set random seed\n",
    "rnd.seed(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 125097\n",
      "hath beaten down young hotspur and his troops,\n"
     ]
    }
   ],
   "source": [
    "dirname = 'data/'\n",
    "lines = [] \n",
    "for filename in os.listdir(dirname):\n",
    "    with open(os.path.join(dirname, filename)) as files:\n",
    "        for line in files:\n",
    "            # remove leading and trailing whitespace\n",
    "            pure_line = line.strip()\n",
    "            # if pure_line is not the empty string,\n",
    "            if pure_line:\n",
    "                lines.append(pure_line.lower())\n",
    "                \n",
    "\n",
    "print(f\"Number of lines: {len(lines)}\")\n",
    "print(lines[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make training and eval from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines for training: 124097\n",
      "Number of lines for validation: 1000\n"
     ]
    }
   ],
   "source": [
    "eval_lines = lines[-1000:] \n",
    "lines = lines[:-1000] \n",
    "\n",
    "print(f\"Number of lines for training: {len(lines)}\")\n",
    "print(f\"Number of lines for validation: {len(eval_lines)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make tensor from each letter(convert to ascii code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_to_tensor(line, EOS_int=1):\n",
    "    \n",
    "    tensor = []\n",
    "    # for each character:\n",
    "    for c in line:\n",
    "        c_int = ord(c)\n",
    "        tensor.append(c_int)\n",
    "\n",
    "    tensor.append(EOS_int)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[73,\n",
       " 32,\n",
       " 108,\n",
       " 105,\n",
       " 107,\n",
       " 101,\n",
       " 32,\n",
       " 100,\n",
       " 101,\n",
       " 101,\n",
       " 112,\n",
       " 108,\n",
       " 101,\n",
       " 97,\n",
       " 114,\n",
       " 110,\n",
       " 105,\n",
       " 110,\n",
       " 103,\n",
       " 33,\n",
       " 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_to_tensor('I like deeplearning!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data_generator(batch_size, max_length, data_lines, line_to_tensor=line_to_tensor):\n",
    "    index =[]\n",
    "    while True:\n",
    "        if len(index)<len(data_lines):\n",
    "            #find index of lines less than maxlength\n",
    "            index = numpy.where([1 if len(line)<max_length else 0 for line in data_lines])[0]\n",
    "        batch_index = numpy.random.choice(index,batch_size)\n",
    "        #remove used index \n",
    "        index = [x for x in index if x not in batch_index]\n",
    "        #make a batch\n",
    "        batch = [data_lines[i] for i in batch_index]  \n",
    "\n",
    "        batch_ = []\n",
    "        mask = []\n",
    "        # make a tensor\n",
    "        for li in batch:\n",
    "            tensor = line_to_tensor(li)\n",
    "            pad = [0] * (max_length - len(tensor))\n",
    "            tensor_pad = tensor + pad\n",
    "            example_mask = [0 if t == 0 else 1 for t in tensor_pad]\n",
    "            mask.append(example_mask)\n",
    "            batch_.append(tensor_pad)\n",
    "        batch_np_arr = np.array(batch_)\n",
    "        mask_np_arr = np.array(mask)\n",
    "        yield batch_np_arr,batch_np_arr,mask_np_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DeviceArray([[49, 50, 51, 52, 53, 54, 55, 56, 57,  1,  0,  0,  0,  0,  0],\n",
       "              [50, 51, 52, 53, 54, 57, 48,  1,  0,  0,  0,  0,  0,  0,  0]],            dtype=int32),\n",
       " DeviceArray([[49, 50, 51, 52, 53, 54, 55, 56, 57,  1,  0,  0,  0,  0,  0],\n",
       "              [50, 51, 52, 53, 54, 57, 48,  1,  0,  0,  0,  0,  0,  0,  0]],            dtype=int32),\n",
       " DeviceArray([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "              [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]], dtype=int32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try out batch data generator\n",
    "tmp_lines = ['12345678901', #length 11\n",
    "             '123456789', # length 9\n",
    "             '2345690', # length 9\n",
    "             '345678901'] # length 9\n",
    "\n",
    "# Get a batch size of 2, max length 10\n",
    "tmp_data_gen = batch_data_generator(batch_size=2, \n",
    "                              max_length=15, \n",
    "                              data_lines=tmp_lines,\n",
    "                            )\n",
    "\n",
    "# get one batch\n",
    "tmp_batch = next(tmp_data_gen)\n",
    "\n",
    "# view the batch\n",
    "tmp_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRULM(vocab_size=256, d_model=512, n_layers=2, mode='train'):\n",
    "    model = tl.Serial(\n",
    "      tl.ShiftRight(mode=mode), # Stack the ShiftRight layer\n",
    "      tl.Embedding(vocab_size=vocab_size, d_feature=d_model), # Stack the embedding layer\n",
    "      [tl.GRU(n_units=d_model) for _ in range(n_layers)], # Stack GRU layers of d_model units keeping n_layer parameter in mind (use list comprehension syntax)\n",
    "      tl.Dense(n_units=vocab_size), # Dense layer\n",
    "      tl.LogSoftmax() # Log Softmax\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial[\n",
      "  Serial[\n",
      "    ShiftRight(1)\n",
      "  ]\n",
      "  Embedding_256_512\n",
      "  GRU_512\n",
      "  GRU_512\n",
      "  Dense_256\n",
      "  LogSoftmax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "model = GRULM()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trax.supervised import training\n",
    "\n",
    "\n",
    "def train_model(model, data_generator, batch_size=32, max_length=64, lines=lines, eval_lines=eval_lines, n_steps=1, output_dir='model/'): \n",
    "\n",
    "    bare_train_generator = data_generator(batch_size, max_length, data_lines=lines)\n",
    "    infinite_train_generator = itertools.cycle(bare_train_generator)\n",
    "    \n",
    "    bare_eval_generator = data_generator(batch_size, max_length, data_lines=eval_lines)\n",
    "    infinite_eval_generator = itertools.cycle(bare_eval_generator)\n",
    "   \n",
    "    train_task = training.TrainTask(\n",
    "        labeled_data=infinite_train_generator, \n",
    "        loss_layer=tl.CrossEntropyLoss(),   \n",
    "        optimizer=trax.optimizers.Adam(0.0005)     \n",
    "    )\n",
    "\n",
    "    eval_task = training.EvalTask(\n",
    "        labeled_data=infinite_eval_generator,    \n",
    "        metrics=[tl.CrossEntropyLoss(), tl.Accuracy()], \n",
    "        n_eval_batches=3      \n",
    "    )\n",
    "    \n",
    "    training_loop = training.Loop(model,\n",
    "                                  tasks = train_task,\n",
    "                                  eval_tasks=eval_task,\n",
    "                                  output_dir=output_dir)\n",
    "\n",
    "    training_loop.run(n_steps=n_steps)\n",
    "    \n",
    "    return training_loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step   5700: Ran 100 train steps in 46.66 secs\n",
      "Step   5700: train CrossEntropyLoss |  1.44672728\n",
      "Step   5700: eval  CrossEntropyLoss |  1.41918472\n",
      "Step   5700: eval          Accuracy |  0.55000877\n",
      "\n",
      "Step   5800: Ran 100 train steps in 43.61 secs\n",
      "Step   5800: train CrossEntropyLoss |  1.38289189\n",
      "Step   5800: eval  CrossEntropyLoss |  1.40104055\n",
      "Step   5800: eval          Accuracy |  0.56479341\n",
      "\n",
      "Step   5900: Ran 100 train steps in 43.71 secs\n",
      "Step   5900: train CrossEntropyLoss |  1.35343218\n",
      "Step   5900: eval  CrossEntropyLoss |  1.38423947\n",
      "Step   5900: eval          Accuracy |  0.56091684\n",
      "\n",
      "Step   6000: Ran 100 train steps in 43.90 secs\n",
      "Step   6000: train CrossEntropyLoss |  1.34320128\n",
      "Step   6000: eval  CrossEntropyLoss |  1.33690719\n",
      "Step   6000: eval          Accuracy |  0.57423945\n",
      "\n",
      "Step   6100: Ran 100 train steps in 44.13 secs\n",
      "Step   6100: train CrossEntropyLoss |  1.32550573\n",
      "Step   6100: eval  CrossEntropyLoss |  1.36520954\n",
      "Step   6100: eval          Accuracy |  0.56659261\n",
      "\n",
      "Step   6200: Ran 100 train steps in 43.77 secs\n",
      "Step   6200: train CrossEntropyLoss |  1.32417667\n",
      "Step   6200: eval  CrossEntropyLoss |  1.38262256\n",
      "Step   6200: eval          Accuracy |  0.57995224\n",
      "\n",
      "Step   6300: Ran 100 train steps in 43.73 secs\n",
      "Step   6300: train CrossEntropyLoss |  1.31493938\n",
      "Step   6300: eval  CrossEntropyLoss |  1.31953764\n",
      "Step   6300: eval          Accuracy |  0.57087553\n",
      "\n",
      "Step   6400: Ran 100 train steps in 43.83 secs\n",
      "Step   6400: train CrossEntropyLoss |  1.31043077\n",
      "Step   6400: eval  CrossEntropyLoss |  1.31438776\n",
      "Step   6400: eval          Accuracy |  0.57809659\n",
      "\n",
      "Step   6500: Ran 100 train steps in 43.75 secs\n",
      "Step   6500: train CrossEntropyLoss |  1.30912304\n",
      "Step   6500: eval  CrossEntropyLoss |  1.30137483\n",
      "Step   6500: eval          Accuracy |  0.58225769\n",
      "\n",
      "Step   6600: Ran 100 train steps in 43.63 secs\n",
      "Step   6600: train CrossEntropyLoss |  1.30692434\n",
      "Step   6600: eval  CrossEntropyLoss |  1.36796371\n",
      "Step   6600: eval          Accuracy |  0.56572590\n"
     ]
    }
   ],
   "source": [
    "training_loop = train_model(GRULM(),batch_data_generator,n_steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$P(W) = \\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}$$\n",
    "\n",
    "As an implementation hack, you would usually take the log of that formula (to enable us to use the log probabilities we get as output of our `RNN`, convert exponents to products, and products into sums which makes computations less complicated and computationally more efficient). You should also take care of the padding, since you do not want to include the padding when calculating the perplexity (because we do not want to have a perplexity measure artificially good).\n",
    "\n",
    "\n",
    "$$log P(W) = {log\\big(\\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}\\big)}$$\n",
    "\n",
    "$$ = {log\\big({\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}\\big)^{\\frac{1}{N}}}$$ \n",
    "\n",
    "$$ = {log\\big({\\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\\big)^{-\\frac{1}{N}}} $$\n",
    "$$ = -\\frac{1}{N}{log\\big({\\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\\big)} $$\n",
    "$$ = -\\frac{1}{N}{\\big({\\sum_{i=1}^{N}{logP(w_i| w_1,...,w_{n-1})}}\\big)} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(preds, target):\n",
    "\n",
    "    total_log_ppx = np.sum(preds * tl.one_hot(target, preds.shape[-1]),axis= -1) \n",
    "\n",
    "    non_pad = 1.0 - np.equal(target, 0)          \n",
    "    ppx = total_log_ppx * non_pad                    \n",
    "\n",
    "    log_ppx = np.sum(ppx) / np.sum(non_pad)\n",
    " \n",
    "    \n",
    "    return -log_ppx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log perplexity and perplexity of your model are respectively 1.3317634 3.7877166\n",
      "The log perplexity and perplexity of your model are respectively 1.2793124 3.5941675\n",
      "The log perplexity and perplexity of your model are respectively 1.3148134 3.724056\n"
     ]
    }
   ],
   "source": [
    "model = GRULM()\n",
    "batch_size =32\n",
    "max_length=64\n",
    "model.init_from_file('model/model.pkl.gz')\n",
    "for x in range(3):\n",
    "    batch = next(batch_data_generator(batch_size, max_length, lines))\n",
    "    preds = model(batch[0])\n",
    "    log_ppx = test_model(preds, batch[1])\n",
    "    print('The log perplexity and perplexity of your model are respectively', log_ppx, np.exp(log_ppx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-17.353485  -13.051613  -17.484028  ... -16.761162  -16.356499\n",
      "   -17.164349 ]\n",
      "  [-19.33699   -13.383178  -18.502068  ... -18.001366  -19.357914\n",
      "   -18.92427  ]\n",
      "  [-14.89674   -13.089214  -14.404482  ... -14.211534  -14.934781\n",
      "   -12.932111 ]\n",
      "  ...\n",
      "  [-19.580896   -7.1406326 -20.074245  ... -17.992218  -18.432888\n",
      "   -19.617348 ]\n",
      "  [-19.65902    -7.682177  -20.06006   ... -17.952805  -18.853691\n",
      "   -19.984495 ]\n",
      "  [-20.172676   -7.265751  -20.601837  ... -18.207088  -18.812744\n",
      "   -20.027431 ]]\n",
      "\n",
      " [[-17.353485  -13.051613  -17.484028  ... -16.761162  -16.356499\n",
      "   -17.164349 ]\n",
      "  [-18.151478   -8.387393  -17.373232  ... -17.808964  -17.810823\n",
      "   -18.81025  ]\n",
      "  [-17.926193  -13.913287  -18.117655  ... -16.872896  -16.792788\n",
      "   -17.548904 ]\n",
      "  ...\n",
      "  [-19.338638  -11.687966  -19.283993  ... -17.402496  -18.97757\n",
      "   -19.385492 ]\n",
      "  [-19.442974  -11.361465  -19.691381  ... -17.903664  -18.74846\n",
      "   -19.415577 ]\n",
      "  [-19.249218  -11.773383  -19.554504  ... -17.796429  -19.018864\n",
      "   -19.419933 ]]\n",
      "\n",
      " [[-17.353485  -13.051613  -17.484028  ... -16.761162  -16.356499\n",
      "   -17.164349 ]\n",
      "  [-17.814127  -10.471663  -17.927639  ... -17.429781  -18.345312\n",
      "   -16.641575 ]\n",
      "  [-19.957619  -17.173918  -20.056284  ... -19.427944  -19.274946\n",
      "   -18.37541  ]\n",
      "  ...\n",
      "  [-19.588428  -10.859324  -19.52259   ... -17.859581  -19.09246\n",
      "   -19.714558 ]\n",
      "  [-19.997095  -10.871943  -19.717846  ... -17.565458  -19.24073\n",
      "   -19.854095 ]\n",
      "  [-19.56387   -10.723979  -19.45177   ... -17.80739   -18.969032\n",
      "   -19.612625 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-17.353485  -13.051613  -17.484028  ... -16.761162  -16.356499\n",
      "   -17.164349 ]\n",
      "  [-18.047209  -13.546619  -16.977076  ... -16.063868  -17.515503\n",
      "   -18.00585  ]\n",
      "  [-17.839024  -14.072237  -16.220938  ... -16.773365  -17.761509\n",
      "   -16.717228 ]\n",
      "  ...\n",
      "  [-18.78547    -7.9779615 -18.984993  ... -17.421333  -18.186623\n",
      "   -19.222702 ]\n",
      "  [-19.249733   -7.969323  -19.12218   ... -17.115757  -18.440327\n",
      "   -19.518112 ]\n",
      "  [-18.78561    -7.8053875 -18.984425  ... -17.38325   -18.133522\n",
      "   -19.171272 ]]\n",
      "\n",
      " [[-17.353485  -13.051613  -17.484026  ... -16.761162  -16.356499\n",
      "   -17.164347 ]\n",
      "  [-18.418293  -12.153844  -18.11316   ... -17.00493   -18.954842\n",
      "   -19.091242 ]\n",
      "  [-22.358788  -13.718843  -20.838877  ... -19.840649  -21.34787\n",
      "   -21.371113 ]\n",
      "  ...\n",
      "  [-18.865816   -9.013568  -19.548481  ... -17.660925  -18.179022\n",
      "   -19.319706 ]\n",
      "  [-18.88911    -9.098156  -19.389584  ... -17.453615  -18.346745\n",
      "   -19.376793 ]\n",
      "  [-19.25127    -8.930015  -19.930939  ... -17.830233  -18.38778\n",
      "   -19.482128 ]]\n",
      "\n",
      " [[-17.353485  -13.051613  -17.484026  ... -16.761162  -16.356499\n",
      "   -17.164347 ]\n",
      "  [-18.151478   -8.387393  -17.373232  ... -17.808964  -17.810823\n",
      "   -18.81025  ]\n",
      "  [-17.5215    -11.149027  -19.081917  ... -18.255272  -17.787071\n",
      "   -18.656652 ]\n",
      "  ...\n",
      "  [-19.85233   -12.714104  -20.049818  ... -18.259079  -19.821266\n",
      "   -20.559622 ]\n",
      "  [-19.976984  -12.48262   -20.478056  ... -18.705877  -19.566715\n",
      "   -20.58706  ]\n",
      "  [-19.781534  -12.819862  -20.28867   ... -18.609219  -19.89566\n",
      "   -20.65253  ]]]\n",
      "[[121 111 117 ...   0   0   0]\n",
      " [ 97  32  99 ...   0   0   0]\n",
      " [107 105 110 ...   0   0   0]\n",
      " ...\n",
      " [ 98 117 116 ...   0   0   0]\n",
      " [116 104  97 ...   0   0   0]\n",
      " [ 97 110 116 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "batch = next(batch_data_generator(batch_size, max_length, lines))\n",
    "preds = model(batch[0])\n",
    "print(preds)\n",
    "print(batch[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the language with your own model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_sample(log_probs, temperature=1.0):\n",
    "    \"\"\"Gumbel sampling from a categorical distribution.\"\"\"\n",
    "    u = numpy.random.uniform(low=1e-6, high=1.0 - 1e-6, size=log_probs.shape)\n",
    "    g = -np.log(-np.log(u))\n",
    "    return np.argmax(log_probs + g * temperature, axis=-1)\n",
    "\n",
    "def predict(num_chars, prefix):\n",
    "    inp = [ord(c) for c in prefix]\n",
    "    result = [c for c in prefix]\n",
    "    max_len = len(prefix) + num_chars\n",
    "    for _ in range(num_chars):\n",
    "        cur_inp = np.array(inp + [0] * (max_len - len(inp)))\n",
    "        outp = model(cur_inp[None, :])  # Add batch dim.\n",
    "        next_char = gumbel_sample(outp[0, len(inp)])\n",
    "        inp += [int(next_char)]\n",
    "       \n",
    "        if inp[-1] == 1:\n",
    "            break  # EOS\n",
    "        result.append(chr(int(next_char)))\n",
    "    \n",
    "    return \"\".join(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so upon each subscript, the king\n"
     ]
    }
   ],
   "source": [
    "print(predict(32, \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am happy because need.\n"
     ]
    }
   ],
   "source": [
    "print(predict(64, \"I am happy because\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
