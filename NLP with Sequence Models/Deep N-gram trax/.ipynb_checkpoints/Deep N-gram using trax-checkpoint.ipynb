{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import trax\n",
    "import trax.fastmath.numpy as np\n",
    "import pickle\n",
    "import itertools\n",
    "import numpy\n",
    "import random as rnd\n",
    "from trax import fastmath\n",
    "from trax import layers as tl\n",
    "\n",
    "# set random seed\n",
    "rnd.seed(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 125097\n",
      "hath beaten down young hotspur and his troops,\n"
     ]
    }
   ],
   "source": [
    "dirname = 'data/'\n",
    "lines = [] \n",
    "for filename in os.listdir(dirname):\n",
    "    with open(os.path.join(dirname, filename)) as files:\n",
    "        for line in files:\n",
    "            # remove leading and trailing whitespace\n",
    "            pure_line = line.strip()\n",
    "            # if pure_line is not the empty string,\n",
    "            if pure_line:\n",
    "                lines.append(pure_line.lower())\n",
    "                \n",
    "\n",
    "print(f\"Number of lines: {len(lines)}\")\n",
    "print(lines[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make training and eval from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines for training: 124097\n",
      "Number of lines for validation: 1000\n"
     ]
    }
   ],
   "source": [
    "eval_lines = lines[-1000:] \n",
    "lines = lines[:-1000] \n",
    "\n",
    "print(f\"Number of lines for training: {len(lines)}\")\n",
    "print(f\"Number of lines for validation: {len(eval_lines)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make tensor from each letter(convert to ascii code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_to_tensor(line, EOS_int=1):\n",
    "    \n",
    "    tensor = []\n",
    "    # for each character:\n",
    "    for c in line:\n",
    "        c_int = ord(c)\n",
    "        tensor.append(c_int)\n",
    "\n",
    "    tensor.append(EOS_int)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[73,\n",
       " 32,\n",
       " 108,\n",
       " 105,\n",
       " 107,\n",
       " 101,\n",
       " 32,\n",
       " 100,\n",
       " 101,\n",
       " 101,\n",
       " 112,\n",
       " 108,\n",
       " 101,\n",
       " 97,\n",
       " 114,\n",
       " 110,\n",
       " 105,\n",
       " 110,\n",
       " 103,\n",
       " 33,\n",
       " 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_to_tensor('I like deeplearning!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data_generator(batch_size, max_length, data_lines, line_to_tensor=line_to_tensor):\n",
    "    index =[]\n",
    "    while True:\n",
    "        if len(index)<len(data_lines):\n",
    "            #find index of lines less than maxlength\n",
    "            index = numpy.where([1 if len(line)<max_length else 0 for line in data_lines])[0]\n",
    "        batch_index = numpy.random.choice(index,batch_size)\n",
    "        #remove used index \n",
    "        index = [x for x in index if x not in batch_index]\n",
    "        #make a batch\n",
    "        batch = [data_lines[i] for i in batch_index]  \n",
    "\n",
    "        batch_ = []\n",
    "        mask = []\n",
    "        # make a tensor\n",
    "        for li in batch:\n",
    "            tensor = line_to_tensor(li)\n",
    "            pad = [0] * (max_length - len(tensor))\n",
    "            tensor_pad = tensor + pad\n",
    "            example_mask = [0 if t == 0 else 1 for t in tensor_pad]\n",
    "            mask.append(example_mask)\n",
    "            batch_.append(tensor_pad)\n",
    "        batch_np_arr = np.array(batch_)\n",
    "        mask_np_arr = np.array(mask)\n",
    "        yield batch_np_arr,batch_np_arr,mask_np_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DeviceArray([[49, 50, 51, 52, 53, 54, 55, 56, 57,  1,  0,  0,  0,  0,  0],\n",
       "              [50, 51, 52, 53, 54, 57, 48,  1,  0,  0,  0,  0,  0,  0,  0]],            dtype=int32),\n",
       " DeviceArray([[49, 50, 51, 52, 53, 54, 55, 56, 57,  1,  0,  0,  0,  0,  0],\n",
       "              [50, 51, 52, 53, 54, 57, 48,  1,  0,  0,  0,  0,  0,  0,  0]],            dtype=int32),\n",
       " DeviceArray([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "              [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]], dtype=int32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try out batch data generator\n",
    "tmp_lines = ['12345678901', #length 11\n",
    "             '123456789', # length 9\n",
    "             '2345690', # length 9\n",
    "             '345678901'] # length 9\n",
    "\n",
    "# Get a batch size of 2, max length 10\n",
    "tmp_data_gen = batch_data_generator(batch_size=2, \n",
    "                              max_length=15, \n",
    "                              data_lines=tmp_lines,\n",
    "                            )\n",
    "\n",
    "# get one batch\n",
    "tmp_batch = next(tmp_data_gen)\n",
    "\n",
    "# view the batch\n",
    "tmp_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRULM(vocab_size=256, d_model=512, n_layers=2, mode='train'):\n",
    "    model = tl.Serial(\n",
    "      tl.ShiftRight(mode=mode), # Stack the ShiftRight layer\n",
    "      tl.Embedding(vocab_size=vocab_size, d_feature=d_model), # Stack the embedding layer\n",
    "      [tl.GRU(n_units=d_model) for _ in range(n_layers)], # Stack GRU layers of d_model units keeping n_layer parameter in mind (use list comprehension syntax)\n",
    "      tl.Dense(n_units=vocab_size), # Dense layer\n",
    "      tl.LogSoftmax() # Log Softmax\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial[\n",
      "  Serial[\n",
      "    ShiftRight(1)\n",
      "  ]\n",
      "  Embedding_256_512\n",
      "  GRU_512\n",
      "  GRU_512\n",
      "  Dense_256\n",
      "  LogSoftmax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "model = GRULM()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trax.supervised import training\n",
    "\n",
    "\n",
    "def train_model(model, data_generator, batch_size=32, max_length=64, lines=lines, eval_lines=eval_lines, n_steps=1, output_dir='model/'): \n",
    "\n",
    "    bare_train_generator = data_generator(batch_size, max_length, data_lines=lines)\n",
    "    infinite_train_generator = itertools.cycle(bare_train_generator)\n",
    "    \n",
    "    bare_eval_generator = data_generator(batch_size, max_length, data_lines=eval_lines)\n",
    "    infinite_eval_generator = itertools.cycle(bare_eval_generator)\n",
    "   \n",
    "    train_task = training.TrainTask(\n",
    "        labeled_data=infinite_train_generator, \n",
    "        loss_layer=tl.CrossEntropyLoss(),   \n",
    "        optimizer=trax.optimizers.Adam(0.0005)     \n",
    "    )\n",
    "\n",
    "    eval_task = training.EvalTask(\n",
    "        labeled_data=infinite_eval_generator,    \n",
    "        metrics=[tl.CrossEntropyLoss(), tl.Accuracy()], \n",
    "        n_eval_batches=3      \n",
    "    )\n",
    "    \n",
    "    training_loop = training.Loop(model,\n",
    "                                  tasks = train_task,\n",
    "                                  eval_tasks=eval_task,\n",
    "                                  output_dir=output_dir)\n",
    "\n",
    "    training_loop.run(n_steps=n_steps)\n",
    "    \n",
    "    return training_loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step    100: Ran 99 train steps in 46.47 secs\n",
      "Step    100: train CrossEntropyLoss |  2.76606679\n",
      "Step    100: eval  CrossEntropyLoss |  2.26216833\n",
      "Step    100: eval          Accuracy |  0.33201503\n",
      "\n",
      "Step    200: Ran 100 train steps in 43.88 secs\n",
      "Step    200: train CrossEntropyLoss |  2.13829851\n",
      "Step    200: eval  CrossEntropyLoss |  1.99351176\n",
      "Step    200: eval          Accuracy |  0.40818863\n",
      "\n",
      "Step    300: Ran 100 train steps in 44.03 secs\n",
      "Step    300: train CrossEntropyLoss |  1.97075748\n",
      "Step    300: eval  CrossEntropyLoss |  1.88342321\n",
      "Step    300: eval          Accuracy |  0.43857095\n",
      "\n",
      "Step    400: Ran 100 train steps in 44.07 secs\n",
      "Step    400: train CrossEntropyLoss |  1.86722732\n",
      "Step    400: eval  CrossEntropyLoss |  1.82994707\n",
      "Step    400: eval          Accuracy |  0.44792417\n",
      "\n",
      "Step    500: Ran 100 train steps in 44.01 secs\n",
      "Step    500: train CrossEntropyLoss |  1.79598188\n",
      "Step    500: eval  CrossEntropyLoss |  1.70853364\n",
      "Step    500: eval          Accuracy |  0.47246412\n",
      "\n",
      "Step    600: Ran 100 train steps in 43.66 secs\n",
      "Step    600: train CrossEntropyLoss |  1.74887967\n",
      "Step    600: eval  CrossEntropyLoss |  1.68663903\n",
      "Step    600: eval          Accuracy |  0.48522954\n",
      "\n",
      "Step    700: Ran 100 train steps in 43.62 secs\n",
      "Step    700: train CrossEntropyLoss |  1.69984829\n",
      "Step    700: eval  CrossEntropyLoss |  1.61638192\n",
      "Step    700: eval          Accuracy |  0.49193989\n"
     ]
    }
   ],
   "source": [
    "training_loop = train_model(GRULM(),batch_data_generator,n_steps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$P(W) = \\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}$$\n",
    "\n",
    "As an implementation hack, you would usually take the log of that formula (to enable us to use the log probabilities we get as output of our `RNN`, convert exponents to products, and products into sums which makes computations less complicated and computationally more efficient). You should also take care of the padding, since you do not want to include the padding when calculating the perplexity (because we do not want to have a perplexity measure artificially good).\n",
    "\n",
    "\n",
    "$$log P(W) = {log\\big(\\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}\\big)}$$\n",
    "\n",
    "$$ = {log\\big({\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}\\big)^{\\frac{1}{N}}}$$ \n",
    "\n",
    "$$ = {log\\big({\\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\\big)^{-\\frac{1}{N}}} $$\n",
    "$$ = -\\frac{1}{N}{log\\big({\\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\\big)} $$\n",
    "$$ = -\\frac{1}{N}{\\big({\\sum_{i=1}^{N}{logP(w_i| w_1,...,w_{n-1})}}\\big)} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(preds, target):\n",
    "\n",
    "    total_log_ppx = np.sum(preds * tl.one_hot(target, preds.shape[-1]),axis= -1) \n",
    "\n",
    "    non_pad = 1.0 - np.equal(target, 0)          \n",
    "    ppx = total_log_ppx * non_pad                    \n",
    "\n",
    "    log_ppx = np.sum(ppx) / np.sum(non_pad)\n",
    " \n",
    "    \n",
    "    return -log_ppx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log perplexity and perplexity of your model are respectively 5.4902134 242.30891\n",
      "The log perplexity and perplexity of your model are respectively 5.4907227 242.43234\n",
      "The log perplexity and perplexity of your model are respectively 5.49048 242.3735\n"
     ]
    }
   ],
   "source": [
    "model = GRULM()\n",
    "batch_size =32\n",
    "max_length=64\n",
    "model.init_from_file('model/model.pkl.gz')\n",
    "for x in range(3):\n",
    "    batch = next(batch_data_generator(batch_size, max_length, lines))\n",
    "    preds = model(batch[0])\n",
    "    log_ppx = test_model(preds, batch[1])\n",
    "    print('The log perplexity and perplexity of your model are respectively', log_ppx, np.exp(log_ppx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-5.5451574 -5.537999  -5.5434384 ... -5.5409994 -5.542995  -5.542787 ]\n",
      "  [-5.540304  -5.5368576 -5.5455866 ... -5.5417385 -5.544574  -5.544505 ]\n",
      "  [-5.555124  -5.5306234 -5.5492845 ... -5.5459957 -5.5411053 -5.546066 ]\n",
      "  ...\n",
      "  [-5.5460014 -5.5188766 -5.5379567 ... -5.5289564 -5.5380177 -5.537578 ]\n",
      "  [-5.5460014 -5.518877  -5.5379567 ... -5.528958  -5.538017  -5.537578 ]\n",
      "  [-5.5460005 -5.5188766 -5.537956  ... -5.528958  -5.538016  -5.537577 ]]\n",
      "\n",
      " [[-5.5451574 -5.537999  -5.5434384 ... -5.5409994 -5.542995  -5.542787 ]\n",
      "  [-5.5538745 -5.5180736 -5.5448008 ... -5.5403438 -5.5445185 -5.5436845]\n",
      "  [-5.5485077 -5.5182605 -5.5472755 ... -5.5424924 -5.546113  -5.5459046]\n",
      "  ...\n",
      "  [-5.5460005 -5.518877  -5.537956  ... -5.5289593 -5.538016  -5.537577 ]\n",
      "  [-5.5460005 -5.518877  -5.537956  ... -5.5289593 -5.538016  -5.537577 ]\n",
      "  [-5.5460005 -5.518877  -5.537956  ... -5.5289593 -5.538016  -5.537577 ]]\n",
      "\n",
      " [[-5.5451574 -5.537999  -5.5434384 ... -5.5409994 -5.542995  -5.542787 ]\n",
      "  [-5.540304  -5.5368576 -5.5455866 ... -5.5417385 -5.544574  -5.544505 ]\n",
      "  [-5.5444374 -5.5285296 -5.5391064 ... -5.550315  -5.545374  -5.54238  ]\n",
      "  ...\n",
      "  [-5.5461755 -5.518939  -5.537972  ... -5.5290794 -5.538369  -5.537841 ]\n",
      "  [-5.5461087 -5.5188904 -5.537959  ... -5.5290117 -5.538207  -5.5377297]\n",
      "  [-5.546067  -5.5188723 -5.5379553 ... -5.52898   -5.5381193 -5.5376654]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-5.5451574 -5.537999  -5.5434384 ... -5.5409994 -5.542995  -5.542787 ]\n",
      "  [-5.5550256 -5.5361323 -5.542461  ... -5.5524554 -5.538591  -5.54662  ]\n",
      "  [-5.5542717 -5.5243635 -5.5497236 ... -5.548707  -5.54008   -5.550959 ]\n",
      "  ...\n",
      "  [-5.545956  -5.519033  -5.5379925 ... -5.5288677 -5.5380254 -5.5375814]\n",
      "  [-5.5459805 -5.518957  -5.537974  ... -5.528897  -5.5380125 -5.537579 ]\n",
      "  [-5.545993  -5.518918  -5.5379653 ... -5.5289183 -5.5380096 -5.5375786]]\n",
      "\n",
      " [[-5.5451574 -5.537999  -5.5434384 ... -5.5409994 -5.542995  -5.542787 ]\n",
      "  [-5.554899  -5.528814  -5.544986  ... -5.533678  -5.554456  -5.5450826]\n",
      "  [-5.55063   -5.5204124 -5.555203  ... -5.5372477 -5.5517526 -5.5471225]\n",
      "  ...\n",
      "  [-5.5467777 -5.519793  -5.539715  ... -5.5277987 -5.540137  -5.538721 ]\n",
      "  [-5.546496  -5.5193753 -5.5389204 ... -5.5281634 -5.5391583 -5.538186 ]\n",
      "  [-5.5463233 -5.5191293 -5.538478  ... -5.5284348 -5.538615  -5.5379047]]\n",
      "\n",
      " [[-5.5451574 -5.537999  -5.5434384 ... -5.5409994 -5.542995  -5.542787 ]\n",
      "  [-5.551994  -5.533544  -5.543922  ... -5.5313396 -5.537823  -5.5412364]\n",
      "  [-5.555884  -5.5254507 -5.538317  ... -5.539248  -5.539045  -5.539035 ]\n",
      "  ...\n",
      "  [-5.5461025 -5.5188885 -5.5379477 ... -5.528955  -5.5382066 -5.537746 ]\n",
      "  [-5.546063  -5.5188723 -5.53795   ... -5.5289497 -5.538118  -5.537676 ]\n",
      "  [-5.546038  -5.518868  -5.5379524 ... -5.5289497 -5.5380697 -5.5376344]]]\n",
      "[[116 111  32 ...   0   0   0]\n",
      " [105 116  32 ...   0   0   0]\n",
      " [116 104 114 ...   0   0   0]\n",
      " ...\n",
      " [121 101 116 ...   0   0   0]\n",
      " [103 114 117 ...   0   0   0]\n",
      " [119 104 105 ...   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "batch = next(batch_data_generator(batch_size, max_length, lines))\n",
    "preds = model(batch[0])\n",
    "print(preds)\n",
    "print(batch[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the language with your own model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_sample(log_probs, temperature=1.0):\n",
    "    \"\"\"Gumbel sampling from a categorical distribution.\"\"\"\n",
    "    u = numpy.random.uniform(low=1e-6, high=1.0 - 1e-6, size=log_probs.shape)\n",
    "    g = -np.log(-np.log(u))\n",
    "    return np.argmax(log_probs + g * temperature, axis=-1)\n",
    "\n",
    "def predict(num_chars, prefix):\n",
    "    inp = [ord(c) for c in prefix]\n",
    "    result = [c for c in prefix]\n",
    "    max_len = len(prefix) + num_chars\n",
    "    for _ in range(num_chars):\n",
    "        cur_inp = np.array(inp + [0] * (max_len - len(inp)))\n",
    "        outp = model(cur_inp[None, :])  # Add batch dim.\n",
    "        next_char = gumbel_sample(outp[0, len(inp)])\n",
    "        inp += [int(next_char)]\n",
    "       \n",
    "        if inp[-1] == 1:\n",
    "            break  # EOS\n",
    "        result.append(chr(int(next_char)))\n",
    "    \n",
    "    return \"\".join(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(32, \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(64, \"I am happy because\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
