{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use word embeding method to produce word translation from english to french. as word embeddings are large file i just little pickle file eith specific words.\n",
    "for English word embeding we can use[archive word2vec, GoogleNews-vectors-negative300.bin.gz](https://code.google.com/archive/p/word2vec/)\n",
    "for french we can use this path:[cross_lingual_text_classification]( https://github.com/vjstark/crosslingual_text_classification)\n",
    "in the terminal, type (in one line) curl -o ./wiki.multi.fr.vec https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.fr.vec\n",
    "\n",
    "you can run these lines to produce pickle file. i use pickle files from coursera natural language processing specialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 949 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from nltk) (0.17.0)\n",
      "Collecting regex\n",
      "  Downloading regex-2020.11.13-cp36-cp36m-manylinux2014_x86_64.whl (723 kB)\n",
      "\u001b[K     |████████████████████████████████| 723 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.54.1-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1435513 sha256=d4ba95ac0e71603513e543e76dc3fc46ffadeab8107410534ee1924dbea77304\n",
      "  Stored in directory: /root/.cache/pip/wheels/de/5e/42/64abaeca668161c3e2cecc24f864a8fc421e3d07a104fc8a51\n",
      "Successfully built nltk\n",
      "Installing collected packages: click, regex, tqdm, nltk\n",
      "Successfully installed click-7.1.2 nltk-3.5 regex-2020.11.13 tqdm-4.54.1\n",
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp36-cp36m-manylinux1_x86_64.whl (24.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2 MB 43 kB/s eta 0:00:012\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.4)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-4.0.1.tar.gz (117 kB)\n",
      "\u001b[K     |████████████████████████████████| 117 kB 37 kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-4.0.1-py3-none-any.whl size=115166 sha256=49d6ba1f7249ff6cff9f7d57981883c2113ec8fce83a64b31b25899020077a71\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/5e/70/42adcaea93c80417ec6accf7db1d6d02367ed02f2254cd5eef\n",
      "Successfully built smart-open\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-3.8.3 smart-open-4.0.1\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.18.4)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\",)': /simple/pandas/\u001b[0m\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.1.4-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.5 MB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.4)\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading pytz-2020.4-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[K     |████████████████████████████████| 509 kB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.14.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.1.4 pytz-2020.4\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install gensim\n",
    "!pip install sklearn\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "import time\n",
    "\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from os import getcwd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this code to download and process the full dataset on your local computer\n",
    "\n",
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "# en_embeddings = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary = True)\n",
    "# fr_embeddings = KeyedVectors.load_word2vec_format('./wiki.multi.fr.vec')\n",
    "\n",
    "\n",
    "# # loading the english to french dictionaries\n",
    "# en_fr_train = get_dict('en-fr.train.txt')\n",
    "# print('The length of the english to french training dictionary is', len(en_fr_train))\n",
    "# en_fr_test = get_dict('en-fr.test.txt')\n",
    "# print('The length of the english to french test dictionary is', len(en_fr_train))\n",
    "\n",
    "# english_set = set(en_embeddings.vocab)\n",
    "# french_set = set(fr_embeddings.vocab)\n",
    "# en_embeddings_subset = {}\n",
    "# fr_embeddings_subset = {}\n",
    "# french_words = set(en_fr_train.values())\n",
    "\n",
    "# for en_word in en_fr_train.keys():\n",
    "#     fr_word = en_fr_train[en_word]\n",
    "#     if fr_word in french_set and en_word in english_set:\n",
    "#         en_embeddings_subset[en_word] = en_embeddings[en_word]\n",
    "#         fr_embeddings_subset[fr_word] = fr_embeddings[fr_word]\n",
    "\n",
    "\n",
    "# for en_word in en_fr_test.keys():\n",
    "#     fr_word = en_fr_test[en_word]\n",
    "#     if fr_word in french_set and en_word in english_set:\n",
    "#         en_embeddings_subset[en_word] = en_embeddings[en_word]\n",
    "#         fr_embeddings_subset[fr_word] = fr_embeddings[fr_word]\n",
    "\n",
    "\n",
    "# pickle.dump( en_embeddings_subset, open( \"en_embeddings.p\", \"wb\" ) )\n",
    "# pickle.dump( fr_embeddings_subset, open( \"fr_embeddings.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from now i use small pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embeddings_subset = pickle.load(open(\"../data/en_embeddings.p\", \"rb\"))\n",
    "fr_embeddings_subset = pickle.load(open(\"../data/fr_embeddings.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look at the an la in english and french"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08007812,  0.10498047,  0.04980469,  0.0534668 , -0.06738281,\n",
       "       -0.12060547,  0.03515625, -0.11865234,  0.04394531,  0.03015137,\n",
       "       -0.05688477, -0.07617188,  0.01287842,  0.04980469, -0.08496094,\n",
       "       -0.06347656,  0.00628662, -0.04321289,  0.02026367,  0.01330566,\n",
       "       -0.01953125,  0.09277344, -0.171875  , -0.00131989,  0.06542969,\n",
       "        0.05834961, -0.08251953,  0.0859375 , -0.00318909,  0.05859375,\n",
       "       -0.03491211, -0.0123291 , -0.0480957 , -0.00302124,  0.05639648,\n",
       "        0.01495361, -0.07226562, -0.05224609,  0.09667969,  0.04296875,\n",
       "       -0.03540039, -0.07324219,  0.03271484, -0.06176758,  0.00787354,\n",
       "        0.0035553 , -0.00878906,  0.0390625 ,  0.03833008,  0.04443359,\n",
       "        0.06982422,  0.01263428, -0.00445557, -0.03320312, -0.04272461,\n",
       "        0.09765625, -0.02160645, -0.0378418 ,  0.01190186, -0.01391602,\n",
       "       -0.11328125,  0.09326172, -0.03930664, -0.11621094,  0.02331543,\n",
       "       -0.01599121,  0.02636719,  0.10742188, -0.00466919,  0.09619141,\n",
       "        0.0279541 , -0.05395508,  0.08544922, -0.03686523, -0.02026367,\n",
       "       -0.08544922,  0.125     ,  0.14453125,  0.0267334 ,  0.15039062,\n",
       "        0.05273438, -0.18652344,  0.08154297, -0.01062012, -0.03735352,\n",
       "       -0.07324219, -0.07519531,  0.03613281, -0.13183594,  0.00616455,\n",
       "        0.05078125,  0.04516602,  0.0100708 , -0.15039062, -0.06005859,\n",
       "        0.05761719, -0.00692749,  0.01586914, -0.0213623 ,  0.10351562,\n",
       "       -0.00029182, -0.046875  , -0.01635742, -0.07861328, -0.06933594,\n",
       "        0.01635742, -0.03149414, -0.01373291, -0.03662109, -0.08886719,\n",
       "       -0.0480957 , -0.01318359, -0.07177734,  0.00588989, -0.04614258,\n",
       "        0.03979492,  0.10058594, -0.04931641,  0.07568359,  0.03881836,\n",
       "       -0.16699219, -0.09619141, -0.10107422,  0.02905273, -0.05786133,\n",
       "       -0.01928711, -0.04296875, -0.08398438, -0.01989746,  0.05151367,\n",
       "        0.00848389, -0.03613281, -0.14941406, -0.01855469, -0.03637695,\n",
       "       -0.07666016, -0.03955078, -0.06152344, -0.02001953,  0.04150391,\n",
       "        0.03686523, -0.07226562,  0.00592041, -0.06298828,  0.00738525,\n",
       "       -0.01586914,  0.01611328, -0.01452637,  0.00772095,  0.10107422,\n",
       "       -0.00558472,  0.01428223, -0.07617188,  0.05639648, -0.01293945,\n",
       "        0.03063965, -0.02490234, -0.09863281,  0.0324707 , -0.02807617,\n",
       "       -0.08105469,  0.02062988,  0.01611328, -0.04199219, -0.03491211,\n",
       "       -0.03759766,  0.05493164,  0.01373291,  0.02685547, -0.05859375,\n",
       "       -0.07177734, -0.12011719, -0.02282715, -0.1640625 , -0.00361633,\n",
       "       -0.05981445,  0.07080078, -0.07714844,  0.05175781, -0.04296875,\n",
       "       -0.04833984,  0.0300293 , -0.06591797, -0.03173828, -0.04882812,\n",
       "       -0.03491211,  0.05883789, -0.01464844,  0.18066406,  0.05688477,\n",
       "        0.05249023,  0.05786133,  0.11669922,  0.05200195, -0.0534668 ,\n",
       "        0.01867676, -0.015625  ,  0.00576782, -0.07324219, -0.11621094,\n",
       "        0.04052734,  0.0625    , -0.04321289,  0.01055908,  0.02172852,\n",
       "        0.04248047,  0.03271484,  0.04418945,  0.05761719,  0.02612305,\n",
       "       -0.01831055, -0.02697754, -0.00674438,  0.00509644, -0.11621094,\n",
       "        0.00364685,  0.05761719, -0.05957031, -0.08837891,  0.0135498 ,\n",
       "        0.04541016, -0.04638672, -0.0177002 , -0.0625    ,  0.03442383,\n",
       "       -0.02416992,  0.03088379,  0.09570312,  0.07958984,  0.03930664,\n",
       "        0.0279541 , -0.0859375 ,  0.08105469,  0.06640625, -0.00041962,\n",
       "       -0.06933594,  0.03588867, -0.03417969,  0.04492188, -0.00772095,\n",
       "       -0.00741577, -0.04760742,  0.01397705, -0.09960938,  0.0246582 ,\n",
       "       -0.09960938,  0.11474609,  0.03173828,  0.02209473,  0.07226562,\n",
       "        0.03686523,  0.02563477,  0.01367188, -0.02734375,  0.00592041,\n",
       "       -0.06738281,  0.05053711, -0.02832031, -0.04516602, -0.01733398,\n",
       "        0.02111816,  0.03515625, -0.04296875,  0.06640625,  0.12207031,\n",
       "        0.12353516,  0.0039978 ,  0.04516602, -0.01855469,  0.04833984,\n",
       "        0.04516602,  0.08691406,  0.02941895,  0.03759766,  0.03442383,\n",
       "       -0.07373047, -0.0402832 , -0.14648438, -0.02441406, -0.01953125,\n",
       "        0.0065918 , -0.0018158 , -0.01092529,  0.09326172,  0.06542969,\n",
       "        0.01843262, -0.09326172, -0.01574707, -0.07128906, -0.08935547,\n",
       "       -0.07128906, -0.03015137, -0.01300049,  0.01635742, -0.01831055,\n",
       "        0.01483154,  0.00500488,  0.00366211,  0.04760742, -0.06884766],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_embeddings_subset['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.18250e-03, -9.43867e-04, -8.82648e-03,  3.24623e-02,\n",
       "       -2.18281e-02,  2.98912e-02,  2.29990e-02,  2.80628e-02,\n",
       "        5.87757e-03, -2.56806e-02,  4.21732e-02, -9.37519e-03,\n",
       "        2.86577e-02, -5.08309e-02, -7.13388e-02, -8.34270e-03,\n",
       "       -2.88709e-02, -3.85773e-02,  4.38649e-02,  1.18239e-01,\n",
       "       -8.57782e-02,  2.65617e-02, -1.21373e-01, -1.88254e-02,\n",
       "       -5.46811e-02, -8.06788e-02,  3.16896e-02, -3.78146e-02,\n",
       "        4.16330e-02, -3.09924e-02, -4.15530e-02,  5.95500e-02,\n",
       "       -6.08129e-02,  1.29453e-01,  5.98485e-02, -1.22053e-01,\n",
       "       -4.65893e-02, -1.76105e-02,  8.36952e-02, -3.43169e-02,\n",
       "        1.02949e-02, -3.74583e-02,  1.56218e-02,  2.52615e-02,\n",
       "        3.09000e-02, -6.15672e-03,  1.78421e-02, -7.97611e-03,\n",
       "        4.08416e-02, -3.61498e-02,  5.93975e-02, -6.13706e-02,\n",
       "        2.76527e-02, -4.04968e-02,  4.02123e-02,  4.60480e-02,\n",
       "        3.12373e-02,  4.31485e-02,  4.45423e-03,  9.88624e-05,\n",
       "       -3.46425e-03, -7.78974e-02,  1.54648e-01, -6.86087e-02,\n",
       "        4.52325e-02, -1.44819e-01, -7.86261e-02, -7.18243e-02,\n",
       "       -5.01803e-02, -9.75832e-02, -5.34395e-02,  9.02073e-02,\n",
       "       -3.81883e-02, -1.13480e-01, -5.68386e-02, -5.91484e-02,\n",
       "       -1.26019e-03,  1.01070e-01,  2.68208e-02,  5.06956e-02,\n",
       "        6.00346e-02,  7.74409e-02, -5.61040e-02,  5.44098e-02,\n",
       "       -7.55055e-02, -2.98645e-02, -2.68502e-02,  3.61278e-02,\n",
       "       -3.62457e-02,  5.22597e-02,  5.38365e-02,  1.08371e-01,\n",
       "        6.78186e-02, -7.00722e-02,  7.43035e-02, -6.62201e-02,\n",
       "        1.06870e-02,  2.79691e-02, -3.26633e-03, -4.25280e-02,\n",
       "       -9.28937e-02, -6.31895e-03,  1.44310e-01, -1.25400e-01,\n",
       "       -1.16685e-02,  2.53940e-02, -4.69559e-02,  2.71642e-02,\n",
       "       -1.46400e-02,  1.33371e-02, -2.44887e-02, -3.82446e-02,\n",
       "       -2.43949e-02, -2.10928e-02, -3.89073e-02, -2.47949e-03,\n",
       "        5.40797e-02, -1.35001e-02,  4.24260e-02,  4.02435e-02,\n",
       "        1.19622e-01,  1.72551e-02, -7.77591e-02,  7.70677e-02,\n",
       "        5.39903e-02,  1.48290e-02, -1.14786e-02,  3.96226e-02,\n",
       "        6.34068e-02,  3.08243e-02, -1.00483e-01,  4.84344e-02,\n",
       "       -2.81898e-02, -3.72230e-02, -8.17851e-03,  1.57217e-01,\n",
       "       -2.91976e-02,  9.02530e-02,  4.98912e-02,  8.09812e-02,\n",
       "       -3.76977e-02,  2.46775e-02,  7.72037e-02,  3.18821e-02,\n",
       "        8.99249e-02,  5.84205e-02,  3.41208e-02,  2.76246e-02,\n",
       "        2.22026e-02,  3.28205e-02,  1.41397e-02, -1.06921e-01,\n",
       "       -3.08114e-02, -1.60526e-02, -1.75090e-02, -1.97769e-02,\n",
       "       -1.68273e-02,  1.02950e-02,  6.34315e-02,  5.85020e-03,\n",
       "       -4.03612e-02,  4.62222e-02, -2.31479e-02, -2.15591e-02,\n",
       "        7.29946e-02, -4.83068e-02, -2.59098e-02,  3.81010e-02,\n",
       "        5.24670e-02, -4.87288e-02, -3.59727e-02, -4.73985e-02,\n",
       "        3.14414e-03, -3.67814e-02, -3.96858e-02,  3.91813e-03,\n",
       "        4.48799e-02,  5.32402e-02, -3.10235e-03,  3.36822e-02,\n",
       "        6.30127e-02, -5.04848e-03,  1.42707e-02,  2.84067e-02,\n",
       "        7.78676e-02,  9.43895e-02,  1.87137e-02, -4.95746e-02,\n",
       "        4.46752e-03,  6.55588e-03, -7.15933e-02, -8.42261e-02,\n",
       "        9.26764e-02,  1.14025e-01, -4.54785e-02, -3.01873e-02,\n",
       "       -1.84554e-02, -6.74820e-02, -9.04368e-02,  3.54715e-03,\n",
       "        6.38533e-03,  5.68251e-02, -7.60443e-02,  5.64864e-02,\n",
       "        3.62291e-02, -3.53769e-02, -3.11386e-02, -1.75674e-02,\n",
       "        1.80552e-02, -3.44819e-02,  9.85215e-02,  2.10186e-02,\n",
       "        3.14230e-02,  1.14043e-02, -5.46951e-02, -1.02189e-01,\n",
       "       -7.43853e-03, -8.57716e-02, -1.25572e-02,  6.55862e-02,\n",
       "        1.90377e-02, -1.28279e-02, -3.03209e-02,  4.31458e-02,\n",
       "       -1.37317e-02, -1.78316e-04, -1.39533e-01,  6.43804e-02,\n",
       "        7.36946e-02, -4.69452e-02,  8.92313e-02,  9.83245e-02,\n",
       "       -2.39298e-02,  2.99051e-02, -1.38605e-02, -2.04330e-03,\n",
       "       -5.42123e-02, -5.63839e-02, -2.83625e-02, -1.22703e-02,\n",
       "       -7.47506e-02, -5.07139e-02,  5.61858e-02,  6.70254e-03,\n",
       "        1.01946e-02,  7.48045e-02,  6.59548e-02, -9.26681e-03,\n",
       "        1.09386e-02,  4.78068e-03,  8.45448e-03,  9.81463e-02,\n",
       "        6.79360e-02, -2.54418e-02, -1.28914e-01, -1.39399e-03,\n",
       "       -6.58585e-02, -7.43397e-02,  8.12461e-02, -6.25947e-02,\n",
       "        2.76097e-03,  1.19247e-01,  3.55322e-02, -4.09402e-02,\n",
       "       -4.01827e-02,  7.90198e-02, -3.70183e-04,  1.64888e-02,\n",
       "        1.18278e-02, -6.77377e-03, -8.74498e-02, -1.12267e-01,\n",
       "        1.16907e-02,  1.05920e-01, -7.28045e-02, -8.08299e-02,\n",
       "        2.91518e-02,  1.24673e-01,  2.06159e-02, -2.28965e-02,\n",
       "       -5.88222e-02,  3.86628e-02, -7.13715e-03,  1.55509e-02,\n",
       "        4.44311e-02,  5.35564e-02,  9.65846e-02,  7.42637e-02,\n",
       "       -2.38635e-02,  6.02833e-02,  3.89618e-02, -1.38955e-01,\n",
       "        8.12259e-03,  2.45422e-02, -6.27549e-02, -4.62050e-02,\n",
       "       -8.70064e-02,  1.11644e-01, -5.03964e-02, -6.03421e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_embeddings_subset['la']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict(file_name):\n",
    "    my_file = pd.read_csv(file_name, delimiter=' ')\n",
    "    etof = {}  # the english to french dictionary to be returned\n",
    "    for i in range(len(my_file)):\n",
    "        # indexing into the rows.\n",
    "        en = my_file.loc[i][0]\n",
    "        fr = my_file.loc[i][1]\n",
    "        etof[en] = fr\n",
    "\n",
    "    return etof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the English to French training dictionary is 5000\n",
      "The length of the English to French test dictionary is 5000\n"
     ]
    }
   ],
   "source": [
    "en_fr_train = get_dict('../data/en-fr.train.txt')\n",
    "print('The length of the English to French training dictionary is', len(en_fr_train))\n",
    "en_fr_test = get_dict('../data/en-fr.test.txt')\n",
    "print('The length of the English to French test dictionary is', len(en_fr_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make XR =Y transformation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrices(en_fr, french_vecs, english_vecs):\n",
    "    \n",
    "    X_l = list()\n",
    "    Y_l = list()\n",
    "    \n",
    "    english_words = english_vecs.keys()\n",
    "    french_words = french_vecs.keys()\n",
    "    \n",
    "    for en_word, fr_word in en_fr.items():\n",
    "        \n",
    "        if en_word in english_words and fr_word in french_words:\n",
    "            en_vec = english_vecs[en_word]\n",
    "            fr_vec = french_vecs[fr_word]\n",
    "            X_l.append(en_vec)\n",
    "            Y_l.append(fr_vec)\n",
    "            \n",
    "    X =  np.vstack(X_l)\n",
    "    Y = np.vstack(Y_l)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = get_matrices(en_fr_train, fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4932, 300), (4932, 300))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ argmin_R(XR-Y)$$\n",
    "it is what we want for finding translation matrix\n",
    "The Frobenius norm of a matrix $A$ (assuming it is of dimension $m,n$) is defined as the square root of the sum of the absolute squares of its elements:\n",
    "$$||A||_F = \\sqrt{\\sum_{i=1}^{m} \\sum_{i=1}^{n}|a_{ij}|^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X, Y, R):\n",
    "    diff = np.dot(X,R)-Y\n",
    "    element_wise_squared = diff**2\n",
    "    m = X.shape[0]\n",
    "    loss = np.sum(element_wise_squared)/m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{d}{dR}(\\frac{1}{m}{||XR-Y||_F}^2)= \\frac{2}{m}X^T.(XR-Y)$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, Y, R):\n",
    "    m = X.shape[0]\n",
    "    # gradient is X^T(XR - Y) * 2/m\n",
    "    gradient = np.dot(X.transpose(),np.dot(X,R)-Y)*(2/m)\n",
    "    return gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_embeddings(X, Y, train_steps=100, learning_rate=0.0003):\n",
    "   \n",
    "    # the number of columns in X is the number of dimensions for a word vector (e.g. 300)\n",
    "    # R is a square matrix with length equal to the number of dimensions in th  word embedding\n",
    "    R = np.random.rand(X.shape[1], X.shape[1])\n",
    "\n",
    "    for i in range(train_steps):\n",
    "        if i % 25 == 0:\n",
    "            print(f\"loss at iteration {i} is: {compute_loss(X, Y, R):.4f}\")\n",
    "   \n",
    "        gradient = compute_gradient(X,Y,R)\n",
    "\n",
    "        # update R by subtracting the learning rate times gradient\n",
    "        R -=  learning_rate * gradient\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0 is: 956.6748\n",
      "loss at iteration 25 is: 97.4570\n",
      "loss at iteration 50 is: 26.8328\n",
      "loss at iteration 75 is: 9.8324\n",
      "loss at iteration 100 is: 4.4147\n",
      "loss at iteration 125 is: 2.3543\n",
      "loss at iteration 150 is: 1.4657\n",
      "loss at iteration 175 is: 1.0456\n",
      "loss at iteration 200 is: 0.8329\n",
      "loss at iteration 225 is: 0.7197\n",
      "loss at iteration 250 is: 0.6568\n",
      "loss at iteration 275 is: 0.6209\n",
      "loss at iteration 300 is: 0.5996\n",
      "loss at iteration 325 is: 0.5868\n",
      "loss at iteration 350 is: 0.5790\n",
      "loss at iteration 375 is: 0.5740\n"
     ]
    }
   ],
   "source": [
    "R_train = align_embeddings(X_train, Y_train, train_steps=400, learning_rate=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we can use k nearest neighbor find similar weord vectors in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(A, B):\n",
    "    \n",
    "    dot = np.dot(A, B)\n",
    "    norma = np.linalg.norm(A)\n",
    "    normb = np.linalg.norm(B)\n",
    "    cos = dot / (norma * normb)\n",
    "\n",
    "    return cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor(v, candidates, k=1):\n",
    "   \n",
    "    similarity_l = []\n",
    "\n",
    "    for row in candidates:\n",
    "        cos_similarity = cosine_similarity(v,row)\n",
    "        similarity_l.append(cos_similarity)\n",
    "        \n",
    "    sorted_ids = np.argsort(similarity_l)\n",
    "\n",
    "    k_idx = sorted_ids[-k:]\n",
    "    return k_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for each translation we search whole corpuse to find similar word embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vocabulary(X, target_lng, R):\n",
    "    translations = np.dot(X,R)\n",
    "    num_correct= 0\n",
    "    for i in range(len(translations)):\n",
    "        translations_index = nearest_neighbor(translations[i],target_lng)\n",
    "        \n",
    "        if translations_index == i:\n",
    "            num_correct += 1\n",
    "    \n",
    "    accuracy = num_correct/len(translations)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set is 0.552\n"
     ]
    }
   ],
   "source": [
    "acc = test_vocabulary(X_val, Y_val, R_train)\n",
    "print(f\"accuracy on test set is {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
